{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annikamarie/CalypsoAI/blob/main/CalypsoAI_Challenge_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56L3kNEk-GUp",
        "outputId": "c4e3b09d-4da6-47f8-d18d-569b575bd797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1BR18V7hHjz",
        "outputId": "d0aed6d0-7a8e-405c-8839-e9fe468e773a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "%tensorflow_version 1.15\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.summary.writer.writer import FileWriter\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from tensorflow.contrib.rnn import GRUCell, LSTMCell\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "import sklearn as sk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.insert(0,'/content/gdrive/MyDrive/CalypsoAI')\n",
        "from preprocess import * "
      ],
      "metadata": {
        "id": "xhyK7CUX921D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and select learning model hyperparameters"
      ],
      "metadata": {
        "id": "AQYCGS_fsKrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = json.loads(open('/content/gdrive/My Drive/CalypsoAI/config.json').read())\n",
        "\n",
        "embedding_size = parameters['embedding_dim']\n",
        "n_classes = parameters['n_classes']\n",
        "learning_rate = parameters['learning_rate']\n",
        "n_layers = parameters['n_layers']\n",
        "lstm_size = parameters['lstm_size']\n",
        "hidden_unit = parameters['hidden_unit']\n",
        "epochs = parameters['num_epochs']\n",
        "batch_size= parameters['batch_size']\n",
        "dropout = parameters['dropout_keep_prob']\n",
        "DELTA = parameters['DELTA']\n",
        "\n",
        "print(\"LOADED HYPERPARAMETERS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsJx6ScosDUW",
        "outputId": "86c31088-f469-4e9c-ae68-051ed75d50a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED HYPERPARAMETERS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and preprocess data"
      ],
      "metadata": {
        "id": "nNhHJdj7yCZB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub4DeLPmjI_0",
        "outputId": "9fe71a01-069a-4678-b95b-d5490c4064b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CRITICAL:root:The maximum length is 63\n",
            "INFO:root:x_train: 17231, x_dev: 3041, x_test: 3578\n",
            "INFO:root:y_train: 17231, y_dev: 3041, y_test: 3578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Loaded and preprocessed Data\n",
            "{'genuine': array([1, 0]), 'satire': array([0, 1])}\n",
            "LOADED AND PREPROCESSED DATASET\n"
          ]
        }
      ],
      "source": [
        "path = '/content/gdrive/My Drive/CalypsoAI/Data/Real or satirical headlines.csv'\n",
        "\n",
        "# Tokenize headlines and map to one_hot labels\n",
        "x_, y_, vocabulary,vocabulary_inv,dataframe ,labels, one_hot = load_data(path)\n",
        "print(\"===> Loaded and preprocessed Data\")\n",
        "print(one_hot)   \n",
        "# split data\n",
        "x, x_test, y, y_test = train_test_split(x_, y_, test_size=0.15, random_state=42)\n",
        "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.15)\n",
        "\n",
        "logging.info('x_train: {}, x_dev: {}, x_test: {}'.format(len(x_train), len(x_dev), len(x_test)))\n",
        "logging.info('y_train: {}, y_dev: {}, y_test: {}'.format(len(y_train), len(y_dev), len(y_test)))\n",
        "\n",
        "print(\"LOADED AND PREPROCESSED DATASET\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set sequence length, vocabulary_size and save model path"
      ],
      "metadata": {
        "id": "2IESMxqKOvgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = x_train.shape[1]\n",
        "vocabulary_size = len(vocabulary)\n",
        "MODEL_PATH = '/content/gdrive/My Drive/CalypsoAI/model/'"
      ],
      "metadata": {
        "id": "lcM87dpByATC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Learning Model (LSTM)"
      ],
      "metadata": {
        "id": "fSFb8we0sVVn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ZylDKiJOLH",
        "outputId": "8402ae4f-3412-4cd2-db0d-1b9890a900a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATED PLACEHOLDERS\n",
            "CREATED EMBEDDINGS\n",
            "CREATED LSTM\n",
            "FULLY CONNECTED\n"
          ]
        }
      ],
      "source": [
        "# input, output and dropout placeholders\n",
        "with tf.name_scope('placeholders'):\n",
        "  input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n",
        "  input_y = tf.placeholder(tf.float32, [None, n_classes], name='input_y')\n",
        "  drops = tf.placeholder(tf.float32, name='dropout')\n",
        "  print(\"CREATED PLACEHOLDERS\")\n",
        "\n",
        "def get_a_cell(lstm_size, keep_prob):\n",
        "  lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
        "  drop_out = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=drops)\n",
        "  return drop_out\n",
        "\n",
        "\n",
        "# Setup the embedding matrix for tensorflow\n",
        "with tf.variable_scope(\"embedding\"):\n",
        "    embedding_tf = tf.get_variable(\"embedding\", [vocabulary_size, embedding_size],trainable=True)\n",
        "    embeddings = tf.nn.embedding_lookup(embedding_tf, input_x)\n",
        "    print(\"CREATED EMBEDDINGS\")\n",
        "\n",
        "with tf.name_scope('Lstm'):\n",
        "  cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell(hidden_unit,drops) for _ in range(n_layers)])\n",
        "  lstm_outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=embeddings, dtype=tf.float32)\n",
        "  last_output = state[-1].h\n",
        "  print(\"CREATED LSTM\")\n",
        "\n",
        "# preditctions\n",
        "with tf.name_scope('Fully_connected'):\n",
        "  W = tf.Variable(tf.truncated_normal([hidden_unit, n_classes], stddev=0.1))\n",
        "  b = tf.Variable(tf.constant(0.1, shape=[n_classes]))\n",
        "  output = tf.nn.xw_plus_b(last_output, W, b)\n",
        "  predictions = tf.argmax(input=output, axis=1, name='predictions')\n",
        "  print(\"FULLY CONNECTED\")\n",
        "\n",
        "\n",
        "# Cross-entropy loss and optimizer initialization\n",
        "with tf.name_scope('Loss'):\n",
        "  loss = tf.reduce_mean(input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=input_y))\n",
        "  global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
        "  probs = tf.nn.softmax(output)\n",
        "\n",
        "# Accuracy metrics\n",
        "with tf.name_scope('Accuracy'):\n",
        "  correct_pred = tf.equal(tf.argmax(input=input_y, axis=1),predictions)\n",
        "  accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "  tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "with tf.name_scope('correct_predictions'):\n",
        "  correct_predictions = tf.equal(tf.argmax(input=input_y, axis=1),predictions)\n",
        "  num_correct = tf.reduce_sum(input_tensor=tf.cast(correct_predictions, 'float'), name='correct_predictions')\n",
        "  \n",
        "merged = tf.summary.merge_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training process"
      ],
      "metadata": {
        "id": "mUqr6euDsjka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hBf-TZYwpuE",
        "outputId": "a03bd38d-9076-4660-f37d-083f15b82cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start learning...\n",
            "epoch: 0\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:22<00:00,  5.86it/s]\n",
            "100%|██████████| 23/23 [00:01<00:00, 19.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.670, val_loss: 0.659, acc: 0.624, val_acc: 0.630\n",
            "epoch: 1\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:13<00:00, 10.13it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.487, val_loss: 0.534, acc: 0.713, val_acc: 0.683\n",
            "epoch: 2\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.05it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.485, val_loss: 0.525, acc: 0.791, val_acc: 0.754\n",
            "epoch: 3\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.03it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.436, val_loss: 0.521, acc: 0.820, val_acc: 0.774\n",
            "epoch: 4\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.20it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.430, val_loss: 0.510, acc: 0.830, val_acc: 0.797\n",
            "epoch: 5\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.11it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.438, val_loss: 0.517, acc: 0.848, val_acc: 0.795\n",
            "epoch: 6\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.04it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.351, val_loss: 0.521, acc: 0.859, val_acc: 0.792\n",
            "epoch: 7\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.98it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.426, val_loss: 0.512, acc: 0.858, val_acc: 0.794\n",
            "epoch: 8\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.82it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.426, val_loss: 0.505, acc: 0.855, val_acc: 0.804\n",
            "epoch: 9\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.17it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 23.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.386, val_loss: 0.512, acc: 0.870, val_acc: 0.803\n",
            "epoch: 10\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:10<00:00, 13.22it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.401, val_loss: 0.520, acc: 0.878, val_acc: 0.801\n",
            "epoch: 11\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.23it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.358, val_loss: 0.526, acc: 0.884, val_acc: 0.802\n",
            "epoch: 12\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.97it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 31.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.335, val_loss: 0.532, acc: 0.888, val_acc: 0.801\n",
            "epoch: 13\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.21it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.643, val_loss: 0.656, acc: 0.877, val_acc: 0.680\n",
            "epoch: 14\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.10it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 36.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.597, val_loss: 0.618, acc: 0.675, val_acc: 0.680\n",
            "epoch: 15\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.09it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 36.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.576, val_loss: 0.605, acc: 0.705, val_acc: 0.695\n",
            "epoch: 16\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.09it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.543, val_loss: 0.600, acc: 0.723, val_acc: 0.707\n",
            "epoch: 17\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.09it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 37.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.541, val_loss: 0.596, acc: 0.739, val_acc: 0.712\n",
            "epoch: 18\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.97it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.510, val_loss: 0.569, acc: 0.750, val_acc: 0.743\n",
            "epoch: 19\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.14it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 32.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.418, val_loss: 0.532, acc: 0.840, val_acc: 0.786\n",
            "epoch: 20\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.13it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 32.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.423, val_loss: 0.532, acc: 0.851, val_acc: 0.787\n",
            "epoch: 21\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.12it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.386, val_loss: 0.538, acc: 0.857, val_acc: 0.785\n",
            "epoch: 22\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.21it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.364, val_loss: 0.547, acc: 0.861, val_acc: 0.786\n",
            "epoch: 23\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.99it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.428, val_loss: 0.550, acc: 0.864, val_acc: 0.786\n",
            "epoch: 24\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.19it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.456, val_loss: 0.543, acc: 0.866, val_acc: 0.786\n",
            "epoch: 25\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.10it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.354, val_loss: 0.548, acc: 0.870, val_acc: 0.788\n",
            "epoch: 26\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.10it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.375, val_loss: 0.545, acc: 0.872, val_acc: 0.788\n",
            "epoch: 27\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.17it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.376, val_loss: 0.547, acc: 0.875, val_acc: 0.789\n",
            "epoch: 28\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.04it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.376, val_loss: 0.559, acc: 0.878, val_acc: 0.787\n",
            "epoch: 29\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.19it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.356, val_loss: 0.549, acc: 0.879, val_acc: 0.791\n",
            "epoch: 30\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.23it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.457, val_loss: 0.547, acc: 0.880, val_acc: 0.790\n",
            "epoch: 31\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.09it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.464, val_loss: 0.531, acc: 0.853, val_acc: 0.749\n",
            "epoch: 32\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.19it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.496, val_loss: 0.528, acc: 0.825, val_acc: 0.753\n",
            "epoch: 33\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.06it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.376, val_loss: 0.531, acc: 0.831, val_acc: 0.750\n",
            "epoch: 34\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.07it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.410, val_loss: 0.525, acc: 0.835, val_acc: 0.755\n",
            "epoch: 35\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.15it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.384, val_loss: 0.529, acc: 0.837, val_acc: 0.752\n",
            "epoch: 36\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.18it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.336, val_loss: 0.523, acc: 0.840, val_acc: 0.757\n",
            "epoch: 37\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.77it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.453, val_loss: 0.529, acc: 0.842, val_acc: 0.757\n",
            "epoch: 38\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.02it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.412, val_loss: 0.532, acc: 0.843, val_acc: 0.756\n",
            "epoch: 39\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:13<00:00, 10.24it/s]\n",
            "100%|██████████| 23/23 [00:01<00:00, 17.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.424, val_loss: 0.530, acc: 0.846, val_acc: 0.757\n",
            "epoch: 40\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:14<00:00,  9.19it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.360, val_loss: 0.528, acc: 0.849, val_acc: 0.760\n",
            "epoch: 41\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.87it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 35.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.343, val_loss: 0.527, acc: 0.852, val_acc: 0.758\n",
            "epoch: 42\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.99it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 31.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.432, val_loss: 0.534, acc: 0.853, val_acc: 0.758\n",
            "epoch: 43\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.04it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 32.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.393, val_loss: 0.543, acc: 0.855, val_acc: 0.756\n",
            "epoch: 44\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.06it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.320, val_loss: 0.539, acc: 0.856, val_acc: 0.757\n",
            "epoch: 45\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.86it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.340, val_loss: 0.534, acc: 0.859, val_acc: 0.759\n",
            "epoch: 46\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.77it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.397, val_loss: 0.535, acc: 0.860, val_acc: 0.758\n",
            "epoch: 47\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.02it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.359, val_loss: 0.545, acc: 0.862, val_acc: 0.757\n",
            "epoch: 48\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.80it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.321, val_loss: 0.535, acc: 0.863, val_acc: 0.760\n",
            "epoch: 49\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:10<00:00, 13.30it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.374, val_loss: 0.540, acc: 0.864, val_acc: 0.762\n",
            "epoch: 50\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.86it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.407, val_loss: 0.539, acc: 0.865, val_acc: 0.763\n",
            "epoch: 51\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.03it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.320, val_loss: 0.534, acc: 0.867, val_acc: 0.763\n",
            "epoch: 52\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 15.00it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.351, val_loss: 0.538, acc: 0.868, val_acc: 0.764\n",
            "epoch: 53\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.87it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 34.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.395, val_loss: 0.543, acc: 0.869, val_acc: 0.763\n",
            "epoch: 54\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:09<00:00, 14.82it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 33.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.364, val_loss: 0.537, acc: 0.870, val_acc: 0.765\n",
            "epoch: 55\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.91it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 32.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.381, val_loss: 0.535, acc: 0.871, val_acc: 0.766\n",
            "epoch: 56\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 134/134 [00:08<00:00, 14.98it/s]\n",
            " 70%|██████▉   | 16/23 [00:00<00:00, 34.56it/s]"
          ]
        }
      ],
      "source": [
        "# Batch generators\n",
        "def batch_generator(X, y, batch_size):\n",
        "    \"\"\"batch generator\"\"\"\n",
        "    size = X.shape[0]\n",
        "    X_copy = X.copy()\n",
        "    y_copy = y.copy()\n",
        "    indices = np.arange(size)\n",
        "    np.random.shuffle(indices)\n",
        "    X_copy = X_copy[indices]\n",
        "    y_copy = y_copy[indices]\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i + batch_size <= size:\n",
        "            yield X_copy[i:i + batch_size], y_copy[i:i + batch_size]\n",
        "            i += batch_size\n",
        "        else:\n",
        "            i = 0\n",
        "            indices = np.arange(size)\n",
        "            np.random.shuffle(indices)\n",
        "            X_copy = X_copy[indices]\n",
        "            y_copy = y_copy[indices]\n",
        "            continue\n",
        "            \n",
        "train_batch_generator = batch_generator(x_train, y_train, batch_size)\n",
        "test_batch_generator = batch_generator(x_dev, y_dev, batch_size)\n",
        "predict_generator = batch_generator(x_test, y_test, batch_size)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    print(\"Start learning...\")\n",
        "    for epoch in range(epochs):\n",
        "        loss_train = 0\n",
        "        loss_val = 0\n",
        "        loss_test = 0\n",
        "        accuracy_train = 0\n",
        "        accuracy_val = 0\n",
        "        accuracy_test = 0\n",
        "        train_loss_l = []\n",
        "        val_loss_l = []\n",
        "\n",
        "        print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
        "\n",
        "        # Training\n",
        "        num_batches = x_train.shape[0] // batch_size\n",
        "        for b in tqdm(range(num_batches)):\n",
        "            x_batch, y_batch = next(train_batch_generator)\n",
        "            loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
        "                                                feed_dict={input_x: x_batch,\n",
        "                                                           input_y: y_batch,\n",
        "                                                           drops: dropout})\n",
        "            train_loss_l.append(loss_tr)\n",
        "            accuracy_train += acc\n",
        "            loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
        "        accuracy_train /= num_batches\n",
        "            \n",
        "        # Validation\n",
        "        num_batches = x_dev.shape[0] // batch_size\n",
        "        for b in tqdm(range(num_batches)):\n",
        "            x_batch, y_batch = next(test_batch_generator)\n",
        "            val_loss, val_acc, summary = sess.run([loss, accuracy, merged],\n",
        "                                                     feed_dict={input_x: x_batch,\n",
        "                                                                input_y: y_batch,\n",
        "                                                                drops: dropout})\n",
        "            val_loss_l.append(val_loss)\n",
        "            accuracy_val += val_acc\n",
        "            loss_val += val_loss\n",
        "        accuracy_val /= num_batches\n",
        "        loss_val /= num_batches\n",
        "\n",
        "        print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
        "            loss_train, loss_val, accuracy_train, accuracy_val))\n",
        "\n",
        "    # predict x_test\n",
        "    num_batches = x_test.shape[0] // batch_size\n",
        "    predict_correct = 0\n",
        "    for batch in tqdm(range(num_batches)):\n",
        "        x_batch, yx_batch = next(predict_generator)\n",
        "        y_true = np.argmax(yx_batch,1)\n",
        "        loss_pred, acc_pred, n_correct, y_pred = sess.run([loss,accuracy,num_correct,predictions], feed_dict={\n",
        "            input_x: x_batch,\n",
        "            input_y: y_batch,\n",
        "            drops : dropout })\n",
        "    \n",
        "    saver.save(sess, MODEL_PATH)\n",
        "    print(\"FINISHED TRAINING\")     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot training and validation loss"
      ],
      "metadata": {
        "id": "CP3pOFBvMHPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_l)\n",
        "plt.plot(val_loss_l)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nHU3oTrmMGrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Precision, Recall and F-1 score to evaluate learning model "
      ],
      "metadata": {
        "id": "6fbBuU5FMO7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision\", sk.metrics.precision_score(y_true, y_pred,average='weighted'))\n",
        "print(\"Recall\", sk.metrics.recall_score(y_true, y_pred,average='weighted'))\n",
        "print(\"f1_score\", sk.metrics.f1_score(y_true, y_pred,average='weighted'))"
      ],
      "metadata": {
        "id": "RfjAxwSMLvYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualise classifcation results in confusion matrix "
      ],
      "metadata": {
        "id": "ncE-GVnVMXJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.subplot()\n",
        "conf_mat = sk.metrics.confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(conf_mat, annot=True)\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.xaxis.set_ticklabels(['genuine','satire']); ax.yaxis.set_ticklabels(['genuine','satire']);"
      ],
      "metadata": {
        "id": "uZeUYrPMME8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Additional questions**"
      ],
      "metadata": {
        "id": "PhOjdO1Hu3Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How might you productize such a model to help in the battle against “fake news”?**\n",
        "\n",
        "Content verification is already included in a variety of social media platforms, where post descriptions or blog headlines are verified for their truthfulness (e.g.: covid misinformation check). Similarly, such a feature could be used to verify products used in advertisements by influencers. An example of this would be, a product being advertised that is very similar to a well-known brand and indirectly profiting off that reputation."
      ],
      "metadata": {
        "id": "qWQowEEsxOyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Given your knowledge of the dataset and the model you have created, how would you go about generating satirical headlines to fool your model?**\n",
        "\n",
        "Unfortunately, there is no one obvious and well-working method of augmenting input data to fool an NLP model like there is in computer vision.  Making NLP , including large language models  (e.g.: BERT or GPT-2) safe from adversarial attacks remains an open problem in NLP and more research is needed. However, there are a few methods that can be used to ‘fool’ a learning model that predicts labels based on textual input sequences. There are two main approaches that have been used in recent research to generate adversarial examples for NLP tasks. The first example of this would be to use visual similarity of a sentence, where the only a few characters in the sentence might be changed or including typos that could have been made by a human writing the headline. One disadvantage of this is that one can easily incorporate automatic spelling checks based on rules or other existing approaches. Another example would be to make the new input semantically as similar as possible to the original input headline (e.g.: paraphrasing the headline so that the meaning remains the same and makes grammatical sense). "
      ],
      "metadata": {
        "id": "d8uhVdXexVXA"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CalypsoAI_Challenge_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}